\documentclass{article}

\usepackage[margin=1.0in]{geometry}

\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{url}

\def\dbar{{\mathchar'26\mkern-12mu d}}

\title{Thermal Physics}
\author{Aayush Arya\\ (tomriddle257@gmail.com)}
\date{Last Updated: \today}
\begin{document}
	
	\maketitle
	
	%\tableofcontents
	%\newpage
	\section{What is Heat?}
	Heat is strictly defined as \textit{energy in transit}. It's meaningless to say that a certain object contains $x$ amount of heat. Similarly, \textit{work} can be made sense of only as "mechanical energy in transit". Spontaneously, when placed in thermal contact, heat flows from a hotter object to a colder one. We note that the seemingly unrelated quantity, work done (W), can be converted into heat. \\
	
	"Heat capacity" is not the amount of heat a system can take, but rather the amount of heat (energy) needed to raise the system's temperature by one unit (1K). Mathematically, we can write
	$$ C = \frac{dQ}{dT}$$
	Specific heat capacity, on the other hand, is the amount of energy that must be supplied to a unit mass of a substance to raise its temperature by 1K.
	
	There are some complications when we try to measure C of a substance. Gases expand very quickly and while doing so, do some work on the surrounding. This is compensated by requiring extra energy to raise its temperature. Therefore,
	$$ C_p = \left(\frac{\partial Q}{\partial T}\right)_p $$
	is the heat capacity at constant pressure. Similarly,
	\begin{equation*}
	C_V = \left(\frac{\partial Q}{\partial T}\right)_V
	\end{equation*}
	 describes the heat capacity at constant volume.
	 
	 \section{Temperature and Zeroth Law}
	 Two objects, say A and B, are said to be in "thermal contact" if they can exchange energy. When left in contact, heat flows from the hotter body to the colder body spontaneously. When the two attain the same final temperature $T_f$, the flow of heat from A $\rightarrow$ B becomes equal to that from B $\rightarrow$ A.\\
	 The two bodies are then said to be in \textbf{thermal equilibrium} with each other as the net flow of heat becomes zero. \\
	 
	 The \textbf{Zeroth law of thermodynamics} says that if two bodies, A and B are in thermal equilibrium with each other and B is in thermal equilibrium with a third body, say C, then A and C must also be in thermal equilibrium.\\
	 This is a profound fact because it tells us that we can assign a macroscopic property related to energy flow that can be universally compared. We name this property "temperature".\\
	 
	 \subsection{Thermometers}
	 An instrument for measuring temperature can be made. The reading of a thermometer stops changing over time when the object being measured and the thermometer are in thermal equilibrium.\\
	 The first thermometer was made by Galileo which used water. Farenheit invented alcohol as well as mercury thermometers.\\
	 
	 Other instruments include resistance thermometers (which usually use Platinum due to high chemical resistance). Apart from Pt, doped Germaninum which shows high stability over thermal recycling.
	 Carbon sensors, $RuO_2$ (whose resistance goes up on cooling) are all used.\\
	 In cryogenics, the vapour pressure \textendash temperature relationship of liquid $He$ is very useful.
	 
	% \subsection{Problems with thermometers}
	 There are a few challenges with defining temperature using thermometer scales.
	 
	 \begin{enumerate}
	 	\item Firstly, to make very precise measurements, the heat capacity of the thermometer should be \textit{very low}.
	 	
	 	\item No substance gives a completely linear temperature scale. Mercury solidifies at low temperature and vaporizes at greater temperature.
	 	
	 	\item Theoretical calculations can be made using the ideal gas equation for gas but gases liquify at cryogenic temperatures and depart from ideal gas behavior.
	 \end{enumerate}
	 
	\section{Thermodynamic Equilibrium}
	A system is in thermodynamic equilibrium when its macroscopic quantities such as temperature, pressure and volume stop changing over time. Thus, thermodynamic equilibrium requires mechanical, thermal and even chemical equilibrium.\\
	
	When in a state of equilibrium, we can describe the state of the system using \textbf{functions or variables of state}. A relation between these functions can be called an \textbf{equation of state}. Such an equation is of the form.
	
	$$ f(p,V,T) = 0 $$
	
	\subsection{Functions of state}
	To identify which variables can be used to describe the state of a system, we will take advantage of a mathematical property.\\ \textit{Exact differentials} happen to have a property that their definite integral depends only on the initial and final parameters, not the path.
	
	$$ \Delta f = \int_{x_a}^{x_b} df = f(x_b) - f(x_a)$$ 
	
	A variable, or a \textbf{function of state}) which can be used to describe a thermodynamic system is required to have this property. Therefore, a function which can't be represented using an exact differential strictly \textbf{can't} be a function of state.
	
	\section{First law of thermodynamics}
	Lavoiser (1789) came up with the idea of a 'caloric' fluid which hotter objects would have more of than colder objects. However, only nine years later, Rumford remarked that heat could be produced by friction. In the 1840s, Mayer and Joule independently did experiments which rule out the possibility of such a fluid.
	
	\subsection{Joule's experiments}
	Mayer showed the production of heat by friction using a paper pulp. This showed that mechanical work and heat are related.
	
	\subsection{The statement}
	Mayer and Helmholtz suggested a profound statement
	\begin{center}
	\boxed{\textbf{Energy is conserved and heat and work both are forms of energy}}.
	\end{center}
	$$ \Delta U = \Delta Q + \Delta W $$
	where $\Delta Q$ is energy supplied \textbf{to} the system and $\Delta W$ is work done \textbf{on} the system.\\
	
	In differential form,
	
	$$ dU = \dbar Q + \dbar W $$
 
 	The only kind of work done we're considering here is expansion work.
 	
 	$$ \dbar W = -pdV$$
 	where the negative sign assures that when dV is negative (i.e. when the gas is compressed), the work done on the system is positive. Although it should be noted that this relation is strictly true only for reversible processes.

	\subsection{Heat Capacities of Gases}
	Unless controlled, since gases expand upon heating, they do some work which causes them to lose some internal energy.
	
	From the equations above, we find that
	$$ dU = \dbar Q - pdV $$
	
	rearranging this gives
	
	\begin{equation}
	\label{first}
	 \dbar Q = dU + pdV
	\end{equation}

	
	which suggests that when we are heating a gas, a fraction of the energy is used up in doing expansion work. But if the volume is kept constant (which means $\Delta V$ is kept zero), all of the energy supplied can in principle be used up just to raise its internal energy. Thus specific heat would be different if pressure were kept constant and not volume. \\
	
	We now move on to finding quantitatively the difference between the two heat capacities.
	Since internal energy can be defined using just two state functions (say, $U = U(T, V)$), and that internal energy is a state function, we can write its exact differential as
	
	$$ dU = \left(\frac{\partial U}{\partial T}\right)_VdT + \left(\frac{\partial U}{\partial V}\right)_TdV $$ 
	
	Substituting this into equation (\ref{first}) and rearranging gives us
	
	$$ \dbar Q = \left(\frac{\partial U}{\partial T}\right)_V dT + \left(\left(\frac{\partial U}{\partial V}\right)_T + p\right)dV $$
	Dividing by dT on both sides gives us,
	
	$$ \frac{dQ}{dT} = \left(\frac{\partial U}{\partial T}\right)_V + \left(\left(\frac{\partial U}{\partial V}\right)_T + p\right)\frac{dV}{dT}$$
	
	\section{Thermodynamic Definition of Entropy}
	Since we have established that $$\oint \frac{\dbar Q}{T} = 0$$ we immediately see that the integral on the left hand side of the equation
	
	$$ \int_{A}^{B}\frac{\dbar Q}{T} = S(B) - S(A)$$
	is path independent. Although $\dbar Q$ itself is an inexact differential, the fraction $\frac{\dbar Q}{T}$ is, by the above argument, an exact differential and hence can be used as a function of state. We assign the variable S the name "\textit{entropy}".
	
	In the case of an adiabatic (reversible adiathermal) process, where $\dbar Q = 0$, change in entropy is zero. Therefore, adiabatic proccesses are \textit{isentropic}.
	
	\section{Another View of the Second Law}
	Consider a system undergoing a cyclic process which has two sections; a reversible portion $A \rightarrow B$ and an irreversible $B \rightarrow A$ portion. From Clausius' inequality, we know that the total change in entropy
	$$ \oint \frac{\dbar Q_{total}}{T} \leq 0 $$
	which can be expanded in the form
	$$ \int_{A}^{B}\frac{\dbar Q_{rev}}{T} + \int_{B}^{A}\frac{\dbar Q}{T} \leq 0 $$
	or,
	$$\int_{A}^{B}\frac{\dbar Q_{rev}}{T} - \int_{A}^{B}\frac{\dbar Q}{T} \leq 0 $$
	rearranging gives,
	$$ \int_{A}^{B}\frac{\dbar Q_{rev}}{T} \leq \int_{A}^{B}\frac{\dbar Q}{T}$$ 

	The equality holds trivially in the case when the second process from $B\rightarrow A$ is actually reversible.
	
	For a thermally isolated system, $\dbar Q_{rev} = 0$ for any process and so the inequality becomes
	
	$$dS \geq 0 $$
	
	If the universe is treated as an isolated system, the first two laws of thermodynamics tell us\\
	
	\indent(i) The total energy content of the universe is constant.\\
	\indent(ii) The entropy of the universe tends to a maximum.
	
	\section{Measuring the Entropy of a system}
	We've learnt different statements of the second law of thermodynamics and how they are equivalent. We've seen how it introduces the concept of entropy and suggests that spontaneously, the entropy of a system can only increase. But how does one measure the entropy of a system?
	
	One way to measure the entropy would be to rely on other well-defined macroscopic properties of a system. For example, let's take the heat capacity at constant pressure. \footnote{It should be noted that one could take any other quantity, including heat capacity at constant volume.}
	
	$$C_p = T\left(\frac{\partial S}{\partial T}\right)_p $$
	This gives us
	$$ S = \int\frac{C_p}{T}dT $$
	By writing entropy as an indefinite integral, we run into the requirement of an arbitrary constant of integration. If we could write it as a definite integral from some reference temperature $T_0$ to $T$
	
	$$ S(T) = S(T_0) + \int_{T_0}^{T}\frac{C_p}{T}dT$$
	
	there's still another problem. We need the value of entropy at that reference point. The third law of thermodynamics gives us precisely that for one particular reference point: \textit{absolute zero}
	
	\section{Statements of the Third Law of Thermodynamics}
	After looking at experimental data of electrochemical cell reactions and chemical thermodynamics experiments, Nernst saw a striking pattern in the change in enthalpy and change in the Gibbs' function.
	
	We know that $G = H - TS$.
	For a system in internal equilibrium, such that the temperature is practically constant,
	
	$$ \Delta G = \Delta H - T\Delta S $$
	
	As temperatures were decreased, not only did $\Delta G$ and $"\Delta H$ become closer, they approached each other asymptotically. On the basis of this, Nernst postulated that as $T \rightarrow 0$, $\Delta S$ should approach $0$. Thus, in 1906, he remarked that\\
	
	\noindent\fbox{
		\parbox{\textwidth}{\textbf{Nernst's Statement:} Near absolute zero, all reactions in a system in internal equilibrium take place with no change in entropy.}
	}\\

	Max Planck gave a more generalized statement by making a further hypothesis. Although it was motivated by the idea of perfect crystals, it is believed to hold for any system.\\
	
	\noindent\fbox{
	\parbox{\textwidth}{\textbf{Planck's Statement:} The entropy of all systems in internal equibrlium is the same, which may be taken as zero.}}\\
	
	where the choice of zero was further motivated by the development of statistical mechanics. From the statistical definition of entropy $S = k_B ln(\Omega)$, zero entropy at absolute zero temperature means $\Omega = 1$ where $\Omega$ is the number of microstates in a macrostate. That is, there is only one possible microscopic configuration for the system. Thus when a system finds its ground state at absolute zero, zero entropy means that this ground state is non-degenerate.
	
	For the third law to hold, the system must be in internal equilibrium (every part of it must be in equilibrium with the rest of the parts). $^4He$ and $^3He$ remain liquid even at near zero Kelvin temperatures. Electrons in a metal can be treated as a gas even upto $T=0$. Both of these systems will follow the third law. A system that is not in internal equilibrium would be glass that has frozen-in disorder. However, since the lowest energy phase in a solid is the perfect crystal and the glass phase is higher energy level, the glass will eventually turn to a perfect crystalline phase. But the process is incredibly slow (the idea that cathedral mirror glasses have flown in the past centuries has been debunked).
	
	We now point out a limitation of Planck's statement. If a system contains N spinless atoms with each nucleus having angular momentum quantum number I, the degeneracy of each nucleus is $2I+1$. If $I>0$, the degeneracy won't be one and entropy would be $S = Nk_Bln(2I+1)$. This is resolved as follows:\\
	
	In internal equilibrium, each part of the system interacts and exchanges energy with other parts. Each nucleus, due to the nuclei surrounding it, experiences a tiny but non-zero magnetic field. This leads to the collective excitation of nuclear spins and this increases the degeneracy. The corresponding nuclear spin waves, at the lowest energy level will occupy the highest wavelength. If the nuclei are cooled sufficiently (which requires an extremely low temperature), the nuclei can be forced to occupy only the longest-wavelength mode such that the entropy of the nuclear spin system is zero.\\
	
	But even if we cool the nuclei enough, there will be residual entropy due to the interactions of the individual nucleons. Each of the subsystem (electrons, nuclei, etc.) is very weakly coupled with each other. We need to cool enough so that the entropy of individual subsystems tends to zero. Francis Simon called each of these subsystems "aspects" and his more refined statement of the third law is as follows:\\
	
	\noindent\fbox{
	\parbox{\textwidth}{\textbf{Simon's Statement:} The contribution to the entropy of a system by each aspect of the system wich is in internal thermodynamic equilibrium tends to zero as $T \rightarrow 0$.}}
\end{document}